{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Evidence Retriever.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "apVLCiuhLVsC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Evidence Retrieval Module for fetching evidence from Wikipedia"
      ]
    },
    {
      "metadata": {
        "id": "pCobAbcN27Dc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z73aVZ2bKiWk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mkdir data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LREBjwv3KjnJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "cd data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R69tti-1NvK7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://s3-eu-west-1.amazonaws.com/fever.public/train.jsonl && wget https://s3-eu-west-1.amazonaws.com/fever.public/shared_task_dev.jsonl && wget https://s3-eu-west-1.amazonaws.com/fever.public/shared_task_test.jsonl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6fPrzVgEaUQ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://www.dropbox.com/s/05on2t6edciczx3/valid.tsv && wget https://www.dropbox.com/s/sp81ovwrovozz53/train.tsv && wget https://www.dropbox.com/s/sggvga937n8a2xx/test.tsv && wget https://www.dropbox.com/s/68m38xy1ws5xgpg/liar_evidence.tsv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "POkvcncqpAS3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget https://s3-eu-west-1.amazonaws.com/fever.public/wiki-pages.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zZq3L8NilacW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip wiki-pages.zip "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jYURz5lhjTU5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!wget https://nlp.stanford.edu/software/stanford-postagger-full-2018-02-27.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xImTtu01juyQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!unzip stanford-postagger-full-2018-02-27.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WQ-FGZ1yWFPf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install -U gensim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-qBZTTf3hc4L",
        "colab_type": "code",
        "outputId": "addf984c-702f-4fd5-c344-5589495ca397",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://github.com/facebookresearch/fastText/archive/v0.1.0.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Redirecting output to ‘wget-log.8’.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "WnyNGTjQhzSl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip v0.1.0.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x1j7cP8emEuH",
        "colab_type": "code",
        "outputId": "cfc52225-d960-4d38-a521-a810aa4d03bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "cd fastText-0.1.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data/fastText-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "snY7XVdo68lK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!make"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nPM4dP6D7ETJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8-Zmw9r031HV",
        "colab_type": "code",
        "outputId": "1aa263bc-195f-4acc-cff5-08a184e1cac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "!wget https://codeload.github.com/epfml/sent2vec/zip/master"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Redirecting output to ‘wget-log’.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mNdJN6PD9TIt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!unzip master"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pe0u28gM69lP",
        "colab_type": "code",
        "outputId": "d1dd7274-30b2-48e3-c307-bc8e9b98be8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "cd sent2vec-master/src"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data/fastText-0.1.0/sent2vec-master/src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aiI7mOR95-9b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install cython"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H_CK3esW35oG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!python setup.py build_ext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9imQwt0h6RIn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "57N5Arx37XiT",
        "colab_type": "code",
        "outputId": "db2187a1-57e8-4222-da5d-34e5183fe40f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "cd ../../.."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1EvjFZHwML5c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Code for getting CPU and GPU information."
      ]
    },
    {
      "metadata": {
        "id": "L_R5ZAa3mmAm",
        "colab_type": "code",
        "outputId": "16797a6e-2342-4d2a-a9ba-5a02db192ca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "cell_type": "code",
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gputil) (1.14.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.7)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RVzs5_TSVPLy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import os\n",
        "from os.path import join, exists, split\n",
        "import glob\n",
        "import cv2\n",
        "import json\n",
        "import datetime\n",
        "import re\n",
        "import string\n",
        "import copy\n",
        "import nltk\n",
        "import itertools\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "from keras.utils import to_categorical\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, Lambda, Input, Bidirectional,  Add, CuDNNGRU, Concatenate, Dropout, GRU, LSTM, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D,Conv1D, AveragePooling2D, MaxPooling2D, MaxPooling1D, GlobalMaxPooling2D\n",
        "from keras.models import Model, load_model, Sequential\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.utils import layer_utils, plot_model\n",
        "from keras.utils.data_utils import get_file\n",
        "from keras.applications.imagenet_utils import preprocess_input\n",
        "from keras.applications import InceptionV3\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import backend as K\n",
        "from keras.initializers import glorot_uniform\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras import backend as k\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import skimage\n",
        "import six\n",
        "import scipy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uI6JAJq7-6nm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "983FwiljVyh4",
        "colab_type": "code",
        "outputId": "006fe031-ecb0-40d1-b1bc-1abf9e8cad5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "cell_type": "code",
      "source": [
        "time_t = datetime.datetime.utcnow()\n",
        "dataset=[]\n",
        "with open('shared_task_test.jsonl', 'r') as f:\n",
        "  dataset = f.read().split('\\n')\n",
        "  dataset.pop()\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "  dataset[i] = json.loads(dataset[i])\n",
        "  \n",
        "X_test_claim = []\n",
        "for i in dataset:\n",
        "  X_test_claim.append(i['claim'])\n",
        "  #X_train_label.append(i['label'])\n",
        "del dataset\n",
        "print(datetime.datetime.utcnow() - time_t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0:00:00.121351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1FvNuCQqMUyN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Text Tokenizer"
      ]
    },
    {
      "metadata": {
        "id": "e6anqkPqWOVb",
        "colab_type": "code",
        "outputId": "7075c2b8-469d-4e4d-af71-4e1e71e15942",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "def getWikiPage(i):\n",
        "  path = '../wiki-pages/wiki-'\n",
        "  content=[]\n",
        "  with open(path+'%03d'%i+'.jsonl', 'r') as f:\n",
        "    document = f.read().split('\\n')\n",
        "    document.pop()\n",
        "    for j in range(len(document)):\n",
        "      document[j] = json.loads(document[j])\n",
        "    lines=[]\n",
        "    idd=[]\n",
        "    line_no=[]\n",
        "    for j in document:      \n",
        "      temp = j['lines']\n",
        "      temp = temp.split('\\n')\n",
        "      temp.pop()\n",
        "      for i in range(len(temp)):\n",
        "        if len(temp[i]) < 35 :\n",
        "          continue\n",
        "        lines.append(temp[i])\n",
        "        idd.append(j['id'])\n",
        "        line_no.append(i)\n",
        "  return lines, idd, line_no\n",
        "\n",
        "def printMaxLen():\n",
        "  path = 'wiki-pages/wiki-'\n",
        "  minn = 10000000\n",
        "  maxx = 0\n",
        "  for i in range(1,110):\n",
        "    cnt = 0\n",
        "    with open(path+'%03d'%i+'.jsonl', 'r') as f:\n",
        "      document = f.read().split('\\n')\n",
        "      document.pop()\n",
        "      for j in range(len(document)):\n",
        "        document[j] = json.loads(document[j])\n",
        "      for j in document:\n",
        "        temp = j['text']\n",
        "        temp = sent_tokenize(temp)\n",
        "        for sent in temp:\n",
        "          if len(sent)>40:\n",
        "            cnt += 1\n",
        "    maxx = max(cnt, maxx)\n",
        "    minn = min(cnt, minn)\n",
        "  print(maxx, minn)\n",
        "  return"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4gtkqMhFRw76",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#printMaxLen()\n",
        "#print(preprocess(getWikiPage(3))[0:100])\n",
        "#getWikiPage(7)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Csek4NaThiPV",
        "colab_type": "code",
        "outputId": "70a2dfd0-350d-432e-85dd-e565815dd3c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import pos_tag\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import unicodedata\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "stop = list(set(stopwords.words('english')))\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def strip_accents(text):\n",
        "    \"\"\"\n",
        "    Strip accents from input String.\n",
        "\n",
        "    :param text: The input string.\n",
        "    :type text: String.\n",
        "\n",
        "    :returns: The processed String.\n",
        "    :rtype: String.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        text = unicode(text, 'utf-8')\n",
        "    except (TypeError, NameError): # unicode is a default on python 3 \n",
        "        pass\n",
        "    text = unicodedata.normalize('NFD', text)\n",
        "    text = text.encode('ascii', 'ignore')\n",
        "    text = text.decode(\"utf-8\")\n",
        "    return str(text)\n",
        "def stem_tokens(tokens, stemmer):\n",
        "    stemmed = []\n",
        "    for item in tokens:\n",
        "      stemmed.append(stemmer.stem(item))\n",
        "    return stemmed\n",
        "def lemmatize_tokens(tokens, lemmatizer):\n",
        "    lemmatized=[]\n",
        "    for word, tag in pos_tag(tokens):\n",
        "      temp = '0'\n",
        "      if tag.startswith(\"NN\"):\n",
        "          temp =  lemmatizer.lemmatize(word, pos='n')\n",
        "      elif tag.startswith('VB'):\n",
        "          temp = lemmatizer.lemmatize(word, pos='v')\n",
        "      elif tag.startswith('JJ'):\n",
        "          temp = lemmatizer.lemmatize(word, pos='a')\n",
        "      else:\n",
        "          temp = word\n",
        "      lemmatized.append(temp)\n",
        "    return lemmatized\n",
        "\n",
        "def tokenize(text):\n",
        "    b = ['-lrb-', '-rrb-', '-rsb-', '-lsb-', '-lcb-', '-rcb-']\n",
        "    text = text.lower()\n",
        "    text = re.sub(\"\\'\"+'[A-Za-z]*', '', text)\n",
        "    text = strip_accents(text)\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token for token in tokens if token not in string.punctuation and token not in b and token not in stop and len(token)>2 and not token.isdigit()]\n",
        "    stems = lemmatize_tokens(tokens, lemmatizer)\n",
        "    return stems\n",
        "  \n",
        "def preprocess(listt):\n",
        "    new_list=[]\n",
        "    for i in listt:\n",
        "      z = tokenize(i)\n",
        "      z = ' '.join(x for x in z)\n",
        "      new_list.append(z)\n",
        "    #print('Preprocess done.')\n",
        "    return new_list \n",
        "  \n",
        "def pad_sentence(listt, length):\n",
        "    new_list = []\n",
        "    max = 0\n",
        "    for string in listt:\n",
        "      if len(string)>max:\n",
        "        max = len(string)\n",
        "     \n",
        "    if length < max:\n",
        "      max = length\n",
        "    print(\"Maximum sequence length: \"+str(max))\n",
        "    for string in listt:\n",
        "      z = max - len(string)\n",
        "      if z > 0:\n",
        "        temp = string + ['<PAD/>']*z\n",
        "      else:\n",
        "        temp = string[0:max]\n",
        "      new_list.append(temp)\n",
        "    return new_list\n",
        "  \n",
        "def build_vocab(sentences):\n",
        "    word_counts = Counter(itertools.chain(*sentences))\n",
        "    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
        "    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
        "    print(\"Length of vocabulary: \" + str(len(vocabulary)))\n",
        "    return [vocabulary, vocabulary_inv]  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VYUFOwXxMa3S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Mounting with Google Drive for getting pretrained model"
      ]
    },
    {
      "metadata": {
        "id": "FwoSTrv77pzI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "#!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "#!apt-get update -qq 2>&1 > /dev/null\n",
        "#!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "!wget https://launchpad.net/~alessandro-strada/+archive/ubuntu/google-drive-ocamlfuse-beta/+build/15331130/+files/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
        "!dpkg -i google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
        "!apt-get install -f\n",
        "!apt-get -y install -qq fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pe1cy0SA8CST",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QcoB-c0KdyX6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import sys\n",
        "import re\n",
        "from subprocess import call\n",
        "import numpy as np\n",
        "from nltk import TweetTokenizer\n",
        "from nltk.tokenize.stanford import StanfordTokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OJaCBvayec6C",
        "colab_type": "code",
        "outputId": "df15e849-9f52-40f8-c204-750e7a5f6d6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "cd fastText-0.1.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'data/fastText-0.1.0'\n",
            "/content/data/fastText-0.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tAwe8J_rZkZX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!cp ../drive/colab/wiki_unigrams.bin model.bin"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1TiIowMfMhQe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Loading the pretrained model"
      ]
    },
    {
      "metadata": {
        "id": "8OmIE7q7ekY1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "MODEL_WIKI_UNIGRAMS = 'model.bin'\n",
        "SNLP_TAGGER_JAR = 'stanford-postagger-full-2018-02-27/stanford-postagger.jar' \n",
        "FASTTEXT_EXEC_PATH = './fasttext'\n",
        "\n",
        "def get_embeddings_for_preprocessed_sentences(sentences, model_path, fasttext_exec_path):\n",
        "    \"\"\"Arguments:\n",
        "        - sentences: a list of preprocessed sentences\n",
        "        - model_path: a path to the sent2vec .bin model\n",
        "        - fasttext_exec_path: a path to the fasttext executable\n",
        "    \"\"\"\n",
        "    timestamp = str(time.time())\n",
        "    test_path = '_fasttext.txt'\n",
        "    embeddings_path = '_fasttext_embeddings.txt'\n",
        "    dump_text_to_disk(test_path, sentences)\n",
        "    call(fasttext_exec_path+\n",
        "          ' print-sentence-vectors '+\n",
        "          model_path + ' < '+\n",
        "          test_path + ' > ' +\n",
        "          embeddings_path, shell=True)\n",
        "    embeddings = read_embeddings(embeddings_path)\n",
        "    #os.remove(test_path)\n",
        "    #os.remove(embeddings_path)\n",
        "    assert(len(sentences) == len(embeddings))\n",
        "    return np.array(embeddings)\n",
        "    new = np.zeros((len(embeddings), 600))\n",
        "    \n",
        "    for i in range(len(embeddings)):\n",
        "      r = len(embeddings[i])\n",
        "      for j in range(r):\n",
        "        new[i][j] = embeddings[i][j]\n",
        "    return new\n",
        "\n",
        "def read_embeddings(embeddings_path):\n",
        "    \"\"\"Arguments:\n",
        "        - embeddings_path: path to the embeddings\n",
        "    \"\"\"\n",
        "    with open(embeddings_path, 'r') as in_stream:\n",
        "        embeddings = []\n",
        "        #print(in_stream.read())\n",
        "        for line in in_stream:          \n",
        "            line = line.strip()\n",
        "            #line = re.sub('[A-Za-z]+ ', '', line)\n",
        "            line = line.split('  ')\n",
        "            line = line[1].replace(' ',',')[1:]\n",
        "            line = '['+line+']'\n",
        "            embeddings.append(eval(line))\n",
        "        return embeddings\n",
        "\n",
        "def dump_text_to_disk(file_path, X, Y=None):\n",
        "    \"\"\"Arguments:\n",
        "        - file_path: where to dump the data\n",
        "        - X: list of sentences to dump\n",
        "        - Y: labels, if any\n",
        "    \"\"\"\n",
        "    with open(file_path, 'w') as out_stream:\n",
        "        if Y is not None:\n",
        "            for x, y in zip(X, Y):\n",
        "                out_stream.write('__label__'+str(y)+' '+x+' \\n')\n",
        "        else:\n",
        "            for x in X:\n",
        "                #print(x)\n",
        "                out_stream.write(x+' \\n')\n",
        "\n",
        "def get_sentence_embeddings(sentences, ngram='bigrams', model='concat_wiki_twitter'):\n",
        "    \"\"\" Returns a numpy matrix of embeddings for one of the published models. It\n",
        "    handles tokenization and can be given raw sentences.\n",
        "    Arguments:\n",
        "        - ngram: 'unigrams' or 'bigrams'\n",
        "        - model: 'wiki', 'twitter', or 'concat_wiki_twitter'\n",
        "        - sentences: a list of raw sentences ['Once upon a time', 'This is another sentence.', ...]\n",
        "    \"\"\"\n",
        "    wiki_embeddings = None\n",
        "    twitter_embbedings = None\n",
        "    tokenized_sentences_NLTK_tweets = None\n",
        "    tokenized_sentences_SNLP = None\n",
        "    if model == \"wiki\" or model == 'concat_wiki_twitter':\n",
        "        #tknzr = StanfordTokenizer(SNLP_TAGGER_JAR, encoding='utf-8')\n",
        "        #s = ' <delimiter> '.join(sentences) #just a trick to make things faster\n",
        "        #tokenized_sentences_SNLP = tokenize_sentences(tknzr, [s])\n",
        "        tokenized_sentences_SNLP = preprocess(sentences)\n",
        "        assert(len(tokenized_sentences_SNLP) == len(sentences))\n",
        "        if ngram == 'unigrams':\n",
        "            wiki_embeddings = get_embeddings_for_preprocessed_sentences(tokenized_sentences_SNLP, \\\n",
        "                                     MODEL_WIKI_UNIGRAMS, FASTTEXT_EXEC_PATH)\n",
        "        else:\n",
        "            wiki_embeddings = get_embeddings_for_preprocessed_sentences(tokenized_sentences_SNLP, \\\n",
        "                                     MODEL_WIKI_BIGRAMS, FASTTEXT_EXEC_PATH)\n",
        "    if model == \"twitter\" or model == 'concat_wiki_twitter':\n",
        "        tknzr = TweetTokenizer()\n",
        "        tokenized_sentences_NLTK_tweets = tokenize_sentences(tknzr, sentences)\n",
        "        if ngram == 'unigrams':\n",
        "            twitter_embbedings = get_embeddings_for_preprocessed_sentences(tokenized_sentences_NLTK_tweets, \\\n",
        "                                     MODEL_TWITTER_UNIGRAMS, FASTTEXT_EXEC_PATH)\n",
        "        else:\n",
        "            twitter_embbedings = get_embeddings_for_preprocessed_sentences(tokenized_sentences_NLTK_tweets, \\\n",
        "                                     MODEL_TWITTER_BIGRAMS, FASTTEXT_EXEC_PATH)\n",
        "    if model == \"twitter\":\n",
        "        return twitter_embbedings\n",
        "    elif model == \"wiki\":\n",
        "        return np.array(wiki_embeddings)\n",
        "    elif model == \"concat_wiki_twitter\":\n",
        "        return np.concatenate((wiki_embeddings, twitter_embbedings), axis=1)\n",
        "    sys.exit(-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WELr2fqB6a9n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#printm()\n",
        "import sent2vec\n",
        "model = sent2vec.Sent2vecModel()\n",
        "model.load_model('model.bin')\n",
        "#printm()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yCD-Bpt_WcvH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "doc1=[\"Today is a hot day. I don't feel like going out because i am gonna get sick. so i better stay at home\", \"The franchise began as a pair of video games for the original Game Boy that were developed by Game Freak and published by Nintendo. It spans a video game series, a trading card game, an anime television series, a film series, books, manga comics, music, toys, and merchandise. Pokémon is the highest-grossing media franchise of all time,[9][10] with over ¥6 trillion[11] ($59.1 billion) in total revenue.\", \"Football is a family of team sports that involve, to varying degrees, kicking a ball with a foot to score a goal. Unqualified, the word football is understood to refer to whichever form of football is the most popular in the regional context in which the word appears.\", \"There are a number of references to traditional, ancient, or prehistoric ball games played by indigenous peoples in many different parts of the world.[3][4][5] Contemporary codes of football can be traced back to the codification of these games at English public schools during the nineteenth century.\"]\n",
        "test = [\"pikachu is my most fav animal\"]\n",
        "\n",
        "my_embeddings = model.embed_sentences(test+doc1)\n",
        "print('got')\n",
        "print(my_embeddings.shape)\n",
        "print(my_embeddings[0].shape, my_embeddings[1:].shape)\n",
        "sim = cosine_similarity(my_embeddings[0:2], my_embeddings[1:])\n",
        "print(sim.shape)\n",
        "print(doc1[np.argmax(sim[0])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ohfBwdbuPGcv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "time_t = datetime.datetime.utcnow()\n",
        "def getLiarData(path):\n",
        "  X=[]\n",
        "  dic={}\n",
        "  with open('../liar_evidence.tsv','r', encoding='cp850', errors='replace') as f:\n",
        "    reader = csv.reader(f, delimiter='\\t')\n",
        "    \n",
        "    for row in reader:\n",
        "      dic[int(row[0])]=row[1]\n",
        "      \n",
        "  with open('../'+path+'.tsv', 'r', encoding='cp850', errors='replace') as f:\n",
        "    reader = csv.reader(f, delimiter='\\t')\n",
        "    for row in reader:\n",
        "      temp={}\n",
        "      temp['id'] = int(row[0][:-5])\n",
        "      if temp['id'] not in dic.keys():\n",
        "        continue\n",
        "      label = 0\n",
        "      strr = row[1].lower()\n",
        "      if strr == 'true':\n",
        "        label=0\n",
        "      elif strr == 'mostly-true':\n",
        "        label=1\n",
        "      elif strr == 'half-true':\n",
        "        label=2\n",
        "      elif strr == 'barely-true':\n",
        "        label=3\n",
        "      elif strr == 'false':\n",
        "        label=4\n",
        "      else:\n",
        "        label=5\n",
        "      temp['label'] = label\n",
        "      temp['claim'] = row[2]\n",
        "      \n",
        "      evidences = nltk.sent_tokenize(dic[temp['id']])\n",
        "      claim_embedding = model.embed_sentence(preprocess([temp['claim']])[0])\n",
        "      ev_embeddings = model.embed_sentences(preprocess(evidences))\n",
        "      \n",
        "      sim_array = cosine_similarity(claim_embedding, ev_embeddings)\n",
        "      idx = np.argmax(sim_array)\n",
        "      temp['evidence'] = evidences[idx]\n",
        "      X.append(temp)\n",
        "    print(len(X))\n",
        "    print(datetime.datetime.utcnow() - time_t) \n",
        "  return X\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WBkauj3uY5S3",
        "colab_type": "code",
        "outputId": "084503cc-2f9f-4777-b30d-3e5f5b5761cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "cell_type": "code",
      "source": [
        "X=getLiarData('valid')\n",
        "\n",
        "with open('liar_valid_train.json', 'w') as output:\n",
        "  json.dump(X, output)\n",
        "  \n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "file_metadata = {\n",
        "  'name': 'liar_valid_train.json',\n",
        "  'mimeType': 'application/json'\n",
        "}\n",
        "media = MediaFileUpload('liar_valid_train.json', \n",
        "                        mimetype='application/json',\n",
        "                        resumable=True)\n",
        "created = drive_service.files().create(body=file_metadata,\n",
        "                                       media_body=media,\n",
        "                                       fields='id').execute()\n",
        "print('File ID: {}'.format(created.get('id'))) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1284\n",
            "0:28:50.495775\n",
            "File ID: 182BcM_ErQthPpSLhjyC40YhcZAbYQHL2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DpWma0T6NC4e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conversion of Sentences to vectors and calculating Cosine Similarity"
      ]
    },
    {
      "metadata": {
        "id": "Jhby7KTY6yV6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "time_t = datetime.datetime.utcnow()\n",
        "'''\n",
        "X_claim = np.load('X_claim.npy')\n",
        "X_support = np.load('X_support.npy')\n",
        "X_sim = np.load('X_sim.npy')\n",
        "'''\n",
        "X_claim = preprocess(X_test_claim)\n",
        "X_support = np.ndarray(shape=(len(X_claim), 5), dtype=object)\n",
        "X_id = np.ndarray(shape=(len(X_claim), 5), dtype=object)\n",
        "X_sim = np.zeros((len(X_claim), 5))\n",
        "X_line_no = np.zeros((len(X_claim), 5))\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2), strip_accents='unicode')\n",
        "tfidf_matrix = tfidf_vectorizer.fit(X_claim)\n",
        "#claim_embeddings = model.embed_sentences(preprocess(X_claim))\n",
        "#print(claim_embeddings.shape)\n",
        "for i in range(1,35): \n",
        "  time_tt = datetime.datetime.utcnow()\n",
        "  X_doc1, X_doc_id, X_doc_line = getWikiPage(i)\n",
        "  l = int(len(X_doc1)/7)\n",
        "  X_doc = preprocess(X_doc1)\n",
        "  tfidf_matrix = tfidf_vectorizer.transform(X_claim + X_doc)\n",
        "  print(tfidf_matrix.shape)\n",
        "  \n",
        "  claim_tfidf = tfidf_matrix[0:len(X_claim)]\n",
        "  ev_tfidf = tfidf_matrix[len(X_claim):]\n",
        "   \n",
        "  for k in range(1,8):\n",
        "    ll = l*k\n",
        "    if k==7:\n",
        "      ll = len(X_doc)\n",
        "    sim_array = cosine_similarity(claim_tfidf, ev_tfidf[l*(k-1):ll]) \n",
        "    print(sim_array.shape)\n",
        "    for j in range(len(X_claim)):\n",
        "      temp_sim = np.max(sim_array[j])\n",
        "      temp_support = X_doc[l*(k-1)+np.argmax(sim_array[j])]\n",
        "      temp_id = X_doc_id[l*(k-1)+np.argmax(sim_array[j])]\n",
        "      temp_line_no = X_doc_line[l*(k-1)+np.argmax(sim_array[j])]\n",
        "      for m in range(5):\n",
        "        if temp_sim > X_sim[j][m]:\n",
        "          X_sim[j][m], temp_sim = temp_sim, X_sim[j][m]\n",
        "          X_support[j][m], temp_support = temp_support, X_support[j][m]\n",
        "          X_id[j][m], temp_id = temp_id, X_id[j][m]\n",
        "          X_line_no[j][m], temp_line_no = temp_line_no, X_line_no[j][m]\n",
        "          \n",
        "    del sim_array\n",
        "    \n",
        "  #np.save('X_claim.npy', X_claim)\n",
        "  #np.save('X_support.npy', X_support)\n",
        "  #np.save('X_sim.npy', X_sim)\n",
        "  \n",
        "  print('wiki-page: '+str(i)+ ' time taken: '+str(datetime.datetime.utcnow() - time_tt))\n",
        "  #del tfidf_matrix \n",
        "print(datetime.datetime.utcnow() - time_t) \n",
        " \n",
        "\n",
        "X=[]\n",
        "for i in range(len(X_claim)):\n",
        "  temp={}\n",
        "  temp['claim'] = X_claim[i]\n",
        "  temp['support'] = X_support[i]\n",
        "  temp['sim'] = X_sim[i]\n",
        "  temp['id'] = X_id[i]\n",
        "  temp['line_no'] = X_line_no[i]\n",
        "  X.append(temp)\n",
        "print('size of test: '+str(len(X)))\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    \"\"\" Special json encoder for numpy types \"\"\"\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
        "            np.int16, np.int32, np.int64, np.uint8,\n",
        "            np.uint16, np.uint32, np.uint64)):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, (np.float_, np.float16, np.float32, \n",
        "            np.float64)):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj,(np.ndarray,)): #### This is the fix\n",
        "            return obj.tolist()\n",
        "        return json.JSONEncoder.default(self, obj)\n",
        "\n",
        "dumped = json.dumps(X, cls=NumpyEncoder)\n",
        "\n",
        "with open('notun_test_tfidf.json', 'w') as output:\n",
        "  json.dump(dumped, output)\n",
        "  \n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "file_metadata = {\n",
        "  'name': 'notun_test_tfidf.json',\n",
        "  'mimeType': 'application/json'\n",
        "}\n",
        "media = MediaFileUpload('notun_test_tfidf.json', \n",
        "                        mimetype='application/json',\n",
        "                        resumable=True)\n",
        "created = drive_service.files().create(body=file_metadata,\n",
        "                                       media_body=media,\n",
        "                                       fields='id').execute()\n",
        "print('File ID: {}'.format(created.get('id'))) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A1ovUPKRvrnE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "X=[]\n",
        "for i in range(len(X_claim)):\n",
        "  temp={}\n",
        "  temp['claim'] = X_claim[i]\n",
        "  temp['support'] = X_support[i]\n",
        "  temp['sim'] = X_sim[i]\n",
        "  temp['id'] = X_id[i]\n",
        "  temp['line_no'] = X_line_no[i]\n",
        "  X.append(temp)\n",
        "print('size of test: '+str(len(X)))\n",
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    \"\"\" Special json encoder for numpy types \"\"\"\n",
        "    def default(self, obj):\n",
        "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
        "            np.int16, np.int32, np.int64, np.uint8,\n",
        "            np.uint16, np.uint32, np.uint64)):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, (np.float_, np.float16, np.float32, \n",
        "            np.float64)):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj,(np.ndarray,)): #### This is the fix\n",
        "            return obj.tolist()\n",
        "        return json.JSONEncoder.default(self, obj)\n",
        "\n",
        "dumped = json.dumps(X, cls=NumpyEncoder)\n",
        "\n",
        "with open('notun_test_tfidf.json', 'w') as output:\n",
        "  json.dump(dumped, output)\n",
        "  \n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "\n",
        "drive_service = build('drive', 'v3')\n",
        "file_metadata = {\n",
        "  'name': 'notun_test_tfidf.json',\n",
        "  'mimeType': 'application/json'\n",
        "}\n",
        "media = MediaFileUpload('notun_test_tfidf.json', \n",
        "                        mimetype='application/json',\n",
        "                        resumable=True)\n",
        "created = drive_service.files().create(body=file_metadata,\n",
        "                                       media_body=media,\n",
        "                                       fields='id').execute()\n",
        "print('File ID: {}'.format(created.get('id'))) "
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}